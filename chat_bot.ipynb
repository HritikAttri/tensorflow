{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "chat_bots.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7pFR0T5lQow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjEJDO_4lQo6",
        "colab_type": "text"
      },
      "source": [
        "### Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cwr5y-uLlQo8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('train_qa.txt', mode = 'rb') as f:\n",
        "    train_data = pickle.load(f)\n",
        "    \n",
        "with open('test_qa.txt', mode = 'rb') as f:\n",
        "    test_data = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSU_DcHelQpA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "3da2c791-91f1-4a5f-a7b4-0e3c207822ec"
      },
      "source": [
        "print(len(train_data))\n",
        "print(len(test_data))\n",
        "\n",
        "print(' '.join(train_data[0][0]))\n",
        "print(' '.join(train_data[0][1]))\n",
        "print(train_data[0][2])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000\n",
            "1000\n",
            "Mary moved to the bathroom . Sandra journeyed to the bedroom .\n",
            "Is Sandra in the hallway ?\n",
            "no\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdyhWAOblQpI",
        "colab_type": "text"
      },
      "source": [
        "### Create a vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aufib-MGlQpJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = set()\n",
        "\n",
        "all_data = train_data + test_data\n",
        "\n",
        "for story, question, answer in all_data:\n",
        "    vocab = vocab.union(set(story))\n",
        "    vocab = vocab.union(set(question))\n",
        "\n",
        "vocab.add('no')\n",
        "vocab.add('yes')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GM_4xzpmlQpO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "1ce6aada-5dbc-40a8-bff5-9391ae558f79"
      },
      "source": [
        "print(vocab)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Sandra', 'football', 'took', 'garden', 'put', 'yes', 'left', 'up', 'got', 'John', '.', 'Is', 'grabbed', 'Mary', 'moved', '?', 'dropped', 'kitchen', 'bathroom', 'the', 'went', 'travelled', 'journeyed', 'bedroom', 'down', 'milk', 'there', 'picked', 'Daniel', 'in', 'hallway', 'office', 'back', 'no', 'to', 'discarded', 'apple'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ladfmLomlQpT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# An extra +1 for keras zero padding\n",
        "vocab_len = len(vocab) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rI95eQt2lQpX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len_story = max(len(data[0]) for data in all_data)\n",
        "max_len_question = max(len(data[1]) for data in all_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2oWOMXylQpc",
        "colab_type": "text"
      },
      "source": [
        "### Cleaning and tokenizing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfrobIgvlQpd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        },
        "outputId": "35053e30-f397-4c07-c087-61008823f86a"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFVI0J6olQpi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(filters = [])\n",
        "tokenizer.fit_on_texts(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlnI6R6alQpn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_story_text = []\n",
        "train_question_text = []\n",
        "train_answer_text = []\n",
        "\n",
        "for story, question, answer in train_data:\n",
        "    train_story_text.append(story)\n",
        "    train_question_text.append(question)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLPvN87flQpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_story_seq = tokenizer.texts_to_sequences(train_story_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRgYkSc8lQpw",
        "colab_type": "text"
      },
      "source": [
        "### Padding the train and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP3uINRIlQpy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_data(data):\n",
        "\n",
        "    X_story = []\n",
        "    X_question = []\n",
        "    y = []\n",
        "\n",
        "    for story, question, answer in data:\n",
        "        story_idx = [tokenizer.word_index[word.lower()]for word in story]\n",
        "        question_idx = [tokenizer.word_index[word.lower()] for word in question]\n",
        "        answer_idx = np.zeros(len(tokenizer.word_index) + 1)\n",
        "        answer_idx[tokenizer.word_index[answer]] = 1\n",
        "\n",
        "        X_story.append(story_idx)\n",
        "        X_question.append(question_idx)\n",
        "        y.append(answer_idx)\n",
        "        \n",
        "    return (pad_sequences(X_story, maxlen = max_len_story), pad_sequences(X_question, maxlen = max_len_question), np.array(y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw2lzreXlQp2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_story, train_question, train_answer = pad_data(train_data)\n",
        "test_story, test_question, test_answer = pad_data(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRnGBKoelQp7",
        "colab_type": "text"
      },
      "source": [
        "### Creating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv4WQ1AUlQp8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Input, Activation, Dense, Permute, Dropout\n",
        "from keras.layers import add, dot, concatenate\n",
        "from keras.layers import LSTM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOtOmieXlQqB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "f5547e6a-7f5a-414e-d648-eec393485d01"
      },
      "source": [
        "# Placeholders for inputs\n",
        "story_input = Input((max_len_story, ))\n",
        "question_input = Input((max_len_question, ))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwxIve9klQqF",
        "colab_type": "text"
      },
      "source": [
        "***Input encoder m***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0y_9foZElQqH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "596bf4cc-a36f-491c-e872-c812e3fc1d38"
      },
      "source": [
        "# Input gets embedded into a sequence of vectors\n",
        "# Output -> (samples, max_len_story, embedding_dim)\n",
        "\n",
        "input_encoder_m = Sequential()\n",
        "input_encoder_m.add(Embedding(input_dim = vocab_len, output_dim = 64))\n",
        "input_encoder_m.add(Dropout(0.3))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyFpTxW5lQqL",
        "colab_type": "text"
      },
      "source": [
        "***Input encoder c***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaA9Fj6blQqN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Input gets embedded into a sequence of vectors of size max_len_question\n",
        "# Output -> (samples, max_len_story, max_len_question)\n",
        "\n",
        "input_encoder_c = Sequential()\n",
        "input_encoder_c.add(Embedding(input_dim = vocab_len, output_dim = max_len_question))\n",
        "input_encoder_c.add(Dropout(0.3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gumHKbpolQqS",
        "colab_type": "text"
      },
      "source": [
        "***Question encoder***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "po0HT8EalQqT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Question gets embedded into sequence of vectors\n",
        "# Output -> (samples, max_len_question, embedding_dim)\n",
        "\n",
        "question_encoder = Sequential()\n",
        "question_encoder.add(Embedding(input_dim = vocab_len, output_dim = 64, input_length = max_len_question))\n",
        "question_encoder.add(Dropout(0.3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0KRqg__lQqc",
        "colab_type": "text"
      },
      "source": [
        "### Encode the sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsE4rbpVlQqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encode the input and question sequences(which are indices) to sequences of dense vectors\n",
        "\n",
        "input_encoded_m = input_encoder_m(story_input)\n",
        "input_encoded_c = input_encoder_c(story_input)\n",
        "question_encoded = question_encoder(question_input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7rGb76blQqm",
        "colab_type": "text"
      },
      "source": [
        "***Compute dot product between input_encoded_m and question_encoded***\n",
        "\n",
        "We choose the encoded axis for dot product."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8j26cXu7lQqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "match = dot([input_encoded_m, question_encoded], axes = (2, 2))\n",
        "match = Activation('softmax')(match)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqx0knCIlQqx",
        "colab_type": "text"
      },
      "source": [
        "***Add this match matrix with input_encoded_c***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmcqREsdlQqy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "response = add([match, input_encoded_c]) # Shape -> (samples, max_len_story, max_len_question)\n",
        "response = Permute((2, 1))(response) # Shape -> (samples, max_len_question, max_len_story)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGuPKWWElQq2",
        "colab_type": "text"
      },
      "source": [
        "***Concatenate results***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcweQVtnlQq3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "answer = concatenate([response, question_encoded])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2NWqQ_ulQq7",
        "colab_type": "text"
      },
      "source": [
        "***Reduce the layers***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztT6Os9RlQq9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add LSTM layer\n",
        "answer = LSTM(32)(answer)\n",
        "\n",
        "# Dropout layer\n",
        "answer = Dropout(0.5)(answer)\n",
        "\n",
        "# Decode the output\n",
        "answer = Dense(vocab_len)(answer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y4W3rjslQrB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "be1e648c-4d15-45cc-bce2-30a7f12f3a32"
      },
      "source": [
        "# Convert output to either 0 or 1\n",
        "answer = Activation('softmax')(answer)\n",
        "\n",
        "# Build the model ([Input placeholder], result)\n",
        "model = Model([story_input, question_input], answer)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlv6EhColQrE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "outputId": "55487c65-07d3-4a78-883e-92ac81539774"
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 156)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 6)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential_1 (Sequential)       multiple             2432        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "sequential_3 (Sequential)       (None, 6, 64)        2432        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dot_1 (Dot)                     (None, 156, 6)       0           sequential_1[1][0]               \n",
            "                                                                 sequential_3[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 156, 6)       0           dot_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "sequential_2 (Sequential)       multiple             228         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 156, 6)       0           activation_1[0][0]               \n",
            "                                                                 sequential_2[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 6, 156)       0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 6, 220)       0           permute_1[0][0]                  \n",
            "                                                                 sequential_3[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 32)           32384       concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 32)           0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 38)           1254        dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 38)           0           dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 38,730\n",
            "Trainable params: 38,730\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJrTNlMsmHQZ",
        "colab_type": "text"
      },
      "source": [
        "### Setting up ModelCheckPoint, EarlyStopping and ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2bViUKBmHqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "checkpoint = ModelCheckpoint('./chatbot.h5',\n",
        "                             monitor='val_loss',\n",
        "                             mode=\"min\",\n",
        "                             save_best_only = True,\n",
        "                             verbose=1)\n",
        "\n",
        "earlystop = EarlyStopping(monitor = 'val_loss', \n",
        "                          min_delta = 0, \n",
        "                          patience = 15,\n",
        "                          verbose = 1,\n",
        "                          restore_best_weights = True)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss',\n",
        "                              factor = 0.2,\n",
        "                              patience = 15,\n",
        "                              verbose = 1,\n",
        "                              min_delta = 0.0001)\n",
        "\n",
        "callbacks = [earlystop, checkpoint, reduce_lr]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ_od2DzlQrJ",
        "colab_type": "text"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "elhCz6OilQrK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f98a8ab1-2036-42d4-b49c-763ef3a776eb"
      },
      "source": [
        "history = model.fit([train_story, train_question], train_answer, batch_size = 32, epochs = 50,\n",
        "                     validation_data = ([test_story, test_question], test_answer), callbacks = callbacks)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/50\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "10000/10000 [==============================] - 8s 841us/step - loss: 0.9484 - acc: 0.4915 - val_loss: 0.6946 - val_acc: 0.5030\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69463, saving model to ./chatbot.h5\n",
            "Epoch 2/50\n",
            "10000/10000 [==============================] - 7s 674us/step - loss: 0.7046 - acc: 0.5000 - val_loss: 0.6932 - val_acc: 0.5030\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.69463 to 0.69324, saving model to ./chatbot.h5\n",
            "Epoch 3/50\n",
            "10000/10000 [==============================] - 7s 666us/step - loss: 0.6968 - acc: 0.4968 - val_loss: 0.6962 - val_acc: 0.5030\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.69324\n",
            "Epoch 4/50\n",
            "10000/10000 [==============================] - 7s 676us/step - loss: 0.6953 - acc: 0.5010 - val_loss: 0.6931 - val_acc: 0.5030\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.69324 to 0.69313, saving model to ./chatbot.h5\n",
            "Epoch 5/50\n",
            "10000/10000 [==============================] - 7s 673us/step - loss: 0.6952 - acc: 0.4955 - val_loss: 0.6932 - val_acc: 0.4900\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.69313\n",
            "Epoch 6/50\n",
            "10000/10000 [==============================] - 7s 661us/step - loss: 0.6949 - acc: 0.4946 - val_loss: 0.6942 - val_acc: 0.5030\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.69313\n",
            "Epoch 7/50\n",
            "10000/10000 [==============================] - 7s 670us/step - loss: 0.6945 - acc: 0.4981 - val_loss: 0.6938 - val_acc: 0.5030\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.69313\n",
            "Epoch 8/50\n",
            "10000/10000 [==============================] - 7s 677us/step - loss: 0.6944 - acc: 0.5008 - val_loss: 0.6948 - val_acc: 0.4970\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.69313\n",
            "Epoch 9/50\n",
            "10000/10000 [==============================] - 7s 688us/step - loss: 0.6943 - acc: 0.4965 - val_loss: 0.6939 - val_acc: 0.5030\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.69313\n",
            "Epoch 10/50\n",
            "10000/10000 [==============================] - 7s 701us/step - loss: 0.6944 - acc: 0.4903 - val_loss: 0.6949 - val_acc: 0.5030\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.69313\n",
            "Epoch 11/50\n",
            "10000/10000 [==============================] - 7s 671us/step - loss: 0.6938 - acc: 0.5020 - val_loss: 0.6947 - val_acc: 0.4990\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.69313\n",
            "Epoch 12/50\n",
            "10000/10000 [==============================] - 7s 672us/step - loss: 0.6899 - acc: 0.5222 - val_loss: 0.6847 - val_acc: 0.5290\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.69313 to 0.68475, saving model to ./chatbot.h5\n",
            "Epoch 13/50\n",
            "10000/10000 [==============================] - 7s 669us/step - loss: 0.6628 - acc: 0.6023 - val_loss: 0.6397 - val_acc: 0.6530\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.68475 to 0.63970, saving model to ./chatbot.h5\n",
            "Epoch 14/50\n",
            "10000/10000 [==============================] - 7s 667us/step - loss: 0.6337 - acc: 0.6498 - val_loss: 0.6283 - val_acc: 0.6460\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.63970 to 0.62834, saving model to ./chatbot.h5\n",
            "Epoch 15/50\n",
            "10000/10000 [==============================] - 7s 678us/step - loss: 0.6268 - acc: 0.6583 - val_loss: 0.6246 - val_acc: 0.6580\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.62834 to 0.62456, saving model to ./chatbot.h5\n",
            "Epoch 16/50\n",
            "10000/10000 [==============================] - 7s 671us/step - loss: 0.6156 - acc: 0.6699 - val_loss: 0.6131 - val_acc: 0.6560\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.62456 to 0.61308, saving model to ./chatbot.h5\n",
            "Epoch 17/50\n",
            "10000/10000 [==============================] - 7s 670us/step - loss: 0.6122 - acc: 0.6709 - val_loss: 0.6069 - val_acc: 0.6640\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.61308 to 0.60685, saving model to ./chatbot.h5\n",
            "Epoch 18/50\n",
            "10000/10000 [==============================] - 7s 675us/step - loss: 0.5996 - acc: 0.6854 - val_loss: 0.5916 - val_acc: 0.6790\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.60685 to 0.59158, saving model to ./chatbot.h5\n",
            "Epoch 19/50\n",
            "10000/10000 [==============================] - 7s 673us/step - loss: 0.5831 - acc: 0.6956 - val_loss: 0.5627 - val_acc: 0.7090\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.59158 to 0.56268, saving model to ./chatbot.h5\n",
            "Epoch 20/50\n",
            "10000/10000 [==============================] - 7s 675us/step - loss: 0.5547 - acc: 0.7204 - val_loss: 0.5306 - val_acc: 0.7280\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.56268 to 0.53063, saving model to ./chatbot.h5\n",
            "Epoch 21/50\n",
            "10000/10000 [==============================] - 7s 692us/step - loss: 0.5178 - acc: 0.7470 - val_loss: 0.4936 - val_acc: 0.7490\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.53063 to 0.49363, saving model to ./chatbot.h5\n",
            "Epoch 22/50\n",
            "10000/10000 [==============================] - 7s 681us/step - loss: 0.4950 - acc: 0.7602 - val_loss: 0.4899 - val_acc: 0.7550\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.49363 to 0.48988, saving model to ./chatbot.h5\n",
            "Epoch 23/50\n",
            "10000/10000 [==============================] - 7s 677us/step - loss: 0.4815 - acc: 0.7748 - val_loss: 0.4739 - val_acc: 0.7600\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.48988 to 0.47388, saving model to ./chatbot.h5\n",
            "Epoch 24/50\n",
            "10000/10000 [==============================] - 7s 676us/step - loss: 0.4548 - acc: 0.7848 - val_loss: 0.4632 - val_acc: 0.7720\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.47388 to 0.46324, saving model to ./chatbot.h5\n",
            "Epoch 25/50\n",
            "10000/10000 [==============================] - 7s 678us/step - loss: 0.4462 - acc: 0.7956 - val_loss: 0.4554 - val_acc: 0.8060\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.46324 to 0.45536, saving model to ./chatbot.h5\n",
            "Epoch 26/50\n",
            "10000/10000 [==============================] - 7s 678us/step - loss: 0.4233 - acc: 0.8089 - val_loss: 0.4246 - val_acc: 0.8230\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.45536 to 0.42460, saving model to ./chatbot.h5\n",
            "Epoch 27/50\n",
            "10000/10000 [==============================] - 7s 680us/step - loss: 0.4063 - acc: 0.8239 - val_loss: 0.4374 - val_acc: 0.8040\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.42460\n",
            "Epoch 28/50\n",
            "10000/10000 [==============================] - 7s 670us/step - loss: 0.3979 - acc: 0.8278 - val_loss: 0.4442 - val_acc: 0.8010\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.42460\n",
            "Epoch 29/50\n",
            "10000/10000 [==============================] - 7s 680us/step - loss: 0.3875 - acc: 0.8309 - val_loss: 0.4075 - val_acc: 0.8250\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.42460 to 0.40753, saving model to ./chatbot.h5\n",
            "Epoch 30/50\n",
            "10000/10000 [==============================] - 7s 682us/step - loss: 0.3821 - acc: 0.8370 - val_loss: 0.4256 - val_acc: 0.8210\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.40753\n",
            "Epoch 31/50\n",
            "10000/10000 [==============================] - 7s 670us/step - loss: 0.3821 - acc: 0.8355 - val_loss: 0.3935 - val_acc: 0.8340\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.40753 to 0.39348, saving model to ./chatbot.h5\n",
            "Epoch 32/50\n",
            "10000/10000 [==============================] - 7s 676us/step - loss: 0.3654 - acc: 0.8469 - val_loss: 0.3962 - val_acc: 0.8100\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.39348\n",
            "Epoch 33/50\n",
            "10000/10000 [==============================] - 7s 699us/step - loss: 0.3599 - acc: 0.8493 - val_loss: 0.3774 - val_acc: 0.8350\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.39348 to 0.37740, saving model to ./chatbot.h5\n",
            "Epoch 34/50\n",
            "10000/10000 [==============================] - 7s 690us/step - loss: 0.3576 - acc: 0.8484 - val_loss: 0.3953 - val_acc: 0.8220\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.37740\n",
            "Epoch 35/50\n",
            "10000/10000 [==============================] - 7s 709us/step - loss: 0.3531 - acc: 0.8482 - val_loss: 0.3692 - val_acc: 0.8320\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.37740 to 0.36919, saving model to ./chatbot.h5\n",
            "Epoch 36/50\n",
            "10000/10000 [==============================] - 7s 691us/step - loss: 0.3433 - acc: 0.8534 - val_loss: 0.4052 - val_acc: 0.8060\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.36919\n",
            "Epoch 37/50\n",
            "10000/10000 [==============================] - 7s 677us/step - loss: 0.3424 - acc: 0.8502 - val_loss: 0.3738 - val_acc: 0.8350\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.36919\n",
            "Epoch 38/50\n",
            "10000/10000 [==============================] - 7s 673us/step - loss: 0.3364 - acc: 0.8550 - val_loss: 0.3734 - val_acc: 0.8360\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.36919\n",
            "Epoch 39/50\n",
            "10000/10000 [==============================] - 7s 674us/step - loss: 0.3314 - acc: 0.8574 - val_loss: 0.3735 - val_acc: 0.8320\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.36919\n",
            "Epoch 40/50\n",
            "10000/10000 [==============================] - 7s 683us/step - loss: 0.3324 - acc: 0.8600 - val_loss: 0.3762 - val_acc: 0.8330\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.36919\n",
            "Epoch 41/50\n",
            "10000/10000 [==============================] - 7s 680us/step - loss: 0.3254 - acc: 0.8589 - val_loss: 0.3810 - val_acc: 0.8330\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.36919\n",
            "Epoch 42/50\n",
            "10000/10000 [==============================] - 7s 669us/step - loss: 0.3217 - acc: 0.8599 - val_loss: 0.3929 - val_acc: 0.8390\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.36919\n",
            "Epoch 43/50\n",
            "10000/10000 [==============================] - 7s 675us/step - loss: 0.3252 - acc: 0.8612 - val_loss: 0.3655 - val_acc: 0.8360\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.36919 to 0.36548, saving model to ./chatbot.h5\n",
            "Epoch 44/50\n",
            "10000/10000 [==============================] - 7s 672us/step - loss: 0.3226 - acc: 0.8614 - val_loss: 0.3843 - val_acc: 0.8420\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.36548\n",
            "Epoch 45/50\n",
            "10000/10000 [==============================] - 7s 672us/step - loss: 0.3196 - acc: 0.8640 - val_loss: 0.3684 - val_acc: 0.8420\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.36548\n",
            "Epoch 46/50\n",
            "10000/10000 [==============================] - 7s 674us/step - loss: 0.3141 - acc: 0.8652 - val_loss: 0.3770 - val_acc: 0.8350\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.36548\n",
            "Epoch 47/50\n",
            "10000/10000 [==============================] - 7s 679us/step - loss: 0.3132 - acc: 0.8647 - val_loss: 0.3676 - val_acc: 0.8330\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.36548\n",
            "Epoch 48/50\n",
            "10000/10000 [==============================] - 7s 693us/step - loss: 0.3096 - acc: 0.8666 - val_loss: 0.3688 - val_acc: 0.8400\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.36548\n",
            "Epoch 49/50\n",
            "10000/10000 [==============================] - 7s 681us/step - loss: 0.3104 - acc: 0.8650 - val_loss: 0.3739 - val_acc: 0.8360\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.36548\n",
            "Epoch 50/50\n",
            "10000/10000 [==============================] - 7s 695us/step - loss: 0.3085 - acc: 0.8670 - val_loss: 0.3906 - val_acc: 0.8250\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.36548\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSUR68J5lQrR",
        "colab_type": "text"
      },
      "source": [
        "### Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBbkqIOulQrS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(([test_story, test_question]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N03KH1KulQrY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "79a1747e-b1e1-4d5c-efe6-d946a0198210"
      },
      "source": [
        "story =' '.join(word for word in test_data[0][0])\n",
        "print(story)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mary got the milk there . John moved to the bedroom .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TL6c9vJClQrc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "1da823f5-d1ac-4e3c-ef3e-2aa255fc0637"
      },
      "source": [
        "query = ' '.join(word for word in test_data[0][1])\n",
        "print(query)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is John in the kitchen ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqD4jp0RlQrg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "bba5c9e9-bc6c-4b72-84ef-21ddd21eebe8"
      },
      "source": [
        "print('Actual answer: ',test_data[0][2])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual answer:  no\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eclJ2fOQlQrj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "774350f8-9245-4396-8386-d31fe53a9121"
      },
      "source": [
        "val_max = np.argmax(predictions[0])\n",
        "\n",
        "for key, val in tokenizer.word_index.items():\n",
        "    if val == val_max:\n",
        "        k = key\n",
        "\n",
        "print(\"Predicted answer: \", k)\n",
        "print(\"Confidence in prediction: \", predictions[0][val_max])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted answer:  no\n",
            "Confidence in prediction:  0.9694801\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}